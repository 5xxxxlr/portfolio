<!DOCTYPE HTML>
<html>
	<head>
		<title>Ran Li</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="assets/css/main.css" />
		<link rel="stylesheet" href="assets/css/customization.css" />
		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
	</head>
	<body>
		<!-- Wrapper -->
			<div id="wrapper">
					<div id="main">
						<div class="inner">
							<header>
								<h1>Ollo, </h1>
								<h1>My Name is Ran Li, always doing <a class="bold">idea engineering</a>.</h1>

								<h2 class="bold">
									PROJECTS:
								</h2>

								<section>
									<!-- Excerpt -->
									<article class="box excerpt">
										<a data-href="#" class="image left">
											<video class="embeded-youtube" controls preload="none" poster="./img/so-show.PNG">
												<source src="./videos/Mixed%20Reality%20Social%20Network%20Application%20-%20So-Show.mp4" type="video/mp4">
												Your browser does not support HTML5 video.
											</video>
											<!--<iframe class="embeded-youtube" src="https://www.youtube.com/embed/WzBv4E51Pgo" frameborder="0" allowfullscreen></iframe>-->
										</a>
										<div>
											<header>
												<span class="date">Mixed Reality In-person Social Network Assistance</span>
												<h3><a data-href="#">So-Show</a></h3>
											</header>
											<div>
												<p>
													It's a more intuitive way to interact with people at social events by adding their digital social profile with real person in space.
												</p>
												<p>
													By linking personal mobile device with the system, editing digital profiles through personal mobile devices will be reflected in mixed reality in real time.
												</p>
												<p>
													It creates a new social network experience which real time digital information supplements the previous unknown information to entities in the mixed spaces.
												</p>

											</div>
										</div>
										<div style="clear: both;"></div>
									</article>

									<article class="box excerpt">
										<a data-href="#" class="image left">
											<video class="embeded-youtube" controls preload="none" poster="./img/balance_ball_magic.png">
												<source src="./videos/Real-Virtual-Real%20chain%20Experiment.mp4" type="video/mp4">
												Your browser does not support HTML5 video.
											</video>
											<video class="embeded-youtube" controls preload="none" poster="./img/supermarket_buy_coke.png">
												<source src="./videos/supermarket_buy_coke.mp4" type="video/mp4">
												Your browser does not support HTML5 video.
											</video>
											<!--<iframe class="embeded-youtube" src="https://www.youtube.com/embed/nm5Xtrj8a6Q" frameborder="0" allowfullscreen></iframe>-->
											<!--<iframe class="embeded-youtube" src="https://www.youtube.com/embed/z1w7nz37igw" frameborder="0" allowfullscreen></iframe>-->
										</a>
										<div>
											<header>
												<span class="date">To buy or not to buy Coca? SOMEBODY help me!</span>
												<h3><a data-href="#">Personal-Assistant</a></h3>
											</header>
											<div>
												<p>
													This personal assistant is designed to provide people with a more visual and vivid guidance to help with their personal habit formation.
													I believe this can be a better solution than voice assisant from a user interaction point of view.
													It leverages the immersive interaction experience that AR provides to help people in a more convincing way and a more effective behavior intervention.
												</p>

											</div>
										</div>
										<div style="clear: both;"></div>
									</article>
									
									<article class="box excerpt">
										<a data-href="#" class="image left">
											<video class="embeded-youtube" controls preload="none" poster="./img/balance_ball_magic.png">
												<source src="./videos/Real-Virtual-Real%20chain%20Experiment.mp4" type="video/mp4">
												Your browser does not support HTML5 video.
											</video>
											<video class="embeded-youtube" controls preload="none" poster="./img/balance_ball_magic_explain.png">
												<source src="./videos/Real-Virtual-Real%20chain%20Experiment(explain).mp4" type="video/mp4">
												Your browser does not support HTML5 video.
											</video>
											<!--<iframe class="embeded-youtube" src="https://www.youtube.com/embed/nm5Xtrj8a6Q" frameborder="0" allowfullscreen></iframe>-->
											<!--<iframe class="embeded-youtube" src="https://www.youtube.com/embed/z1w7nz37igw" frameborder="0" allowfullscreen></iframe>-->
										</a>
										<div>
											<header>
												<span class="date">Real-Virtual-Real Chain</span>
												<h3><a data-href="#">Real-Virtual Ballencing</a></h3>
											</header>
											<div>
												<p>
													The project aimed to create an experience which transfers events between reality and virtual space.
													Actions that are initiated from reality to virtual world will create real world effects from virtual space afterwards.
													It's no longer a single direction control always originated from reality anymore.
													Just like the Turning Test is for AI, the ultimate test for mixed reality will be whether we can distinguish which pieces are real which are virtual when seeing dominoes' falling one pushing another.
												</p>

											</div>
										</div>
										<div style="clear: both;"></div>
									</article>

									<article class="box excerpt">
										<a data-href="#" class="image left">
											<video class="embeded-youtube" controls preload="none" poster="./img/hololinge.PNG">
												<source src="./videos/Hololens%20learning%20new%20language.mp4" type="video/mp4">
												Your browser does not support HTML5 video.
											</video>
											<!--<iframe class="embeded-youtube" src="https://www.youtube.com/embed/Cr-W8ZpkiQs" frameborder="0" allowfullscreen></iframe>-->
										</a>
										<div>
											<header>
												<span class="date">Mixed Reality Language Learning Assistance</span>
												<h3><a data-href="#">Hololingo</a></h3>
											</header>
											<div>
												<p>An augmented reality application that helps students to learn language.</p>
												<p> The application is able to take voice input from teachers who setup the language learning goals by looking at the real world objects and binding pronunciations with them.

													After the setup stage, the students will try to look for these objects in the real world by listening and searching the spatial sounds source as guidance.

													It is an effective way of learning language vocabulary by triggering a genuine 'discovering' mindset by leveraging the mixed reality and virtual spacial sounds.
												</p>
											</div>
										</div>
										<div style="clear: both;"></div>
									</article>

									<!--<article class="box excerpt">-->
										<!--<a data-href="#" class="image left">-->
											<!--<img class="embeded-youtube" src="./img/mindful-wearable.PNG">-->
											<!--<iframe class="embeded-youtube" src="https://www.youtube.com/embed/q0ZK8wtPSw8x" frameborder="0" allowfullscreen></iframe>-->
										<!--</a>-->
										<!--<div>-->
											<!--<header>-->
												<!--<span class="date">Behavior Interaction</span>-->
												<!--<h3><a data-href="#">Mindful Wearable</a></h3>-->
											<!--</header>-->
											<!--<div>-->
												<!--<p>-->
													<!--A project that was born in the MIT Media Lab, Fluid Interface group.-->
													<!--This was the early prototype.-->
													<!--It is aimed to analyze the visual input and provide certain intervention/guidance to the users.-->
												<!--</p>-->
											<!--</div>-->
										<!--</div>-->
										<!--<div style="clear: both;"></div>-->
									<!--</article>-->

									<article class="box excerpt">
										<a data-href="#" class="image left">
											<video class="embeded-youtube" controls preload="none" poster="./img/hospital_remote_control.png">
												<source src="./videos/Robot%20Remote%20Control.mp4" type="video/mp4">
												Your browser does not support HTML5 video.
											</video>
											<!--<iframe class="embeded-youtube" src="https://www.youtube.com/embed/XwkInPuPcG0" frameborder="0" allowfullscreen></iframe>-->
										</a>
										<div>
											<header>
												<span class="date">Hospital Remote Control Interface</span>
												<h3><a data-href="#">Home-care-robot</a></h3>
											</header>
											<div>
												<p>
													Robots that are capable of 'sensing' the patients' homes will be deployed to their houses after they are discharged from hospitals for follow-up medical care.
												</p>
												<p>
													The robots' 'sensing' system includes a map-generation system that can create maps of the patients' homes which enable a better observation for hospitals.
													The robots can also 'sense' the patients by a human-tracking system.
												</p>
												<p>
													Through the system, doctors are able to observe multiple patients who are at homes remotely without leaving hospitals and provide help in time.
												</p>
											</div>
										</div>
										<div style="clear: both;"></div>
									</article>


									<article class="box excerpt">
										<a data-href="#" class="image left">
											<video class="embeded-youtube" controls preload="none" poster="./img/emotiv_wheelchair.png">
												<source src="./videos/wheelchair%20emotiv%20demo.mp4" type="video/mp4">
												Your browser does not support HTML5 video.
											</video>
											<video class="embeded-youtube" controls preload="none" poster="./img/wheelchair_path_bag.png">
												<source src="./videos/wheelchair%20path%20bag.mp4" type="video/mp4">
												Your browser does not support HTML5 video.
											</video>
											<video class="embeded-youtube" controls preload="none" poster="./img/emotiv_browser.png">
												<source src="./videos/Emotiv%20control%20browser.mp4" type="video/mp4">
												Your browser does not support HTML5 video.
											</video>
											<video class="embeded-youtube" controls preload="none" poster="./img/emotiv_turtlebot.png">
												<source src="./videos/Emotiv%20control%20Turtlebot.mp4" type="video/mp4">
												Your browser does not support HTML5 video.
											</video>
											<!--<iframe class="embeded-youtube" src="https://www.youtube.com/embed/Sb91fOYLG2g" frameborder="0" allowfullscreen></iframe>-->
											<!--<iframe class="embeded-youtube" src="https://www.youtube.com/embed/Wqu-d__jWl4" frameborder="0" allowfullscreen></iframe>-->
											<!--<iframe class="embeded-youtube" src="https://www.youtube.com/embed/syjQ7-KmUFg" frameborder="0" allowfullscreen></iframe>-->
										</a>
										<div>
											<header>
												<span class="date">EMG Human Computer Interfaces</span>
												<h3><a data-href="#">Adaptive EMG controller</a></h3>
											</header>
											<div>
												<p>
													The project's goal is trying to replace traditional hand control interfaces such as joy sticks,
													keyboards so people with no limb mobility can be better facilitated.
												</p>
												<p>
													The control interface module can take in multiple control signals through facial expressions via Emotive headset toolkit.
													The outputs are processed from the raw signals to filtered results.
												</p>
												<p>
													The modularized interface is adapted to control wheelchair's navigation system so people who can't use their limbs(locked in syndrome) are still able to have certain mobility.

													The wheelchair users are able to navigate through the new interface in relatively complex environments.

													By recording the manipulation data with wheelchair's trajectory, certain optimizations were explored by utilizing these history data.
												</p>
												<p>
													It can also be adapted to personal computers for a web browsing experience.
													It provides certain convenience to people who can't use mice and keyboards.
													Users are able to browse various predefined websites with certain navigation abilities.
												</p>

												<p>
													The same controller can also be easily adapted to become a control interface for robotic systems.
												</p>
											</div>
										</div>
										<div style="clear: both;"></div>
									</article>

									<article class="box excerpt">
										<a data-href="#" class="image left">
											<video class="embeded-youtube" controls preload="none" poster="./img/voice_wheelchair.png">
												<source src="./videos/voice%20control%20wheelchair.mp4" type="video/mp4">
												Your browser does not support HTML5 video.
											</video>
											<!--<iframe class="embeded-youtube" src="https://www.youtube.com/embed/luStbnqcmEg" frameborder="0" allowfullscreen></iframe>-->
										</a>
										<div>
											<header>
												<span class="date">Voice Human Computer Interfaces</span>
												<h3><a data-href="#">Adaptive Voice controller</a></h3>
											</header>
											<div>
												<p>
													A different approach to replace the physical control interfaces (Joystick, keyboard, mouse).
												</p>
											</div>
										</div>
										<div style="clear: both;"></div>
									</article>

									<article class="box excerpt">
										<a data-href="#" class="image left">
											<video class="embeded-youtube" controls preload="none" poster="./img/darpa.png">
												<source src="./videos/Door%20Task%20in%20DRC%20Trials,%202013.mp4" type="video/mp4">
												Your browser does not support HTML5 video.
											</video>
											<!--<iframe class="embeded-youtube" src="https://www.youtube.com/embed/ecN-d_ikPo8" frameborder="0" allowfullscreen></iframe>-->
										</a>
										<div>
											<header>
												<span class="date">US Defense Advanced Research Projects Agency (DARPA) Robotics Challenge</span>
												<h3><a data-href="#">DARPA Atlas</a></h3>
											</header>
											<div>
												<p>
													I worked on the three-door task as part of the DARPA challenge 2013-2014.

													It requires the robot to locate three doors and open them, then go through each one.
												</p>
												<p>
													The task used point cloud data collected through a rotating 360-degree LIDAR sensor.
													The robot was able to identify the doors and their handles' positions.
													Then the human operators are able to control the robot to open and walk across the doors.
												</p>
											</div>
										</div>
										<div style="clear: both;"></div>
									</article>

									<article class="box excerpt">
										<a data-href="#" class="image left">
											<img class="embeded-youtube" src="./img/range_sensor_system.png">
											<img class="embeded-youtube" src="./img/low_cost_lidar.PNG">
										</a>
										<div>
											<header>
												<span class="date">A Low Cost Object Detection System</span>
												<h3><a data-href="#">LIDAR imitation</a></h3>
											</header>
											<div>
												<p>
													A mixed sensor system combining IR sensors and Ultrasonic sensors is built in a highly configurable design that can do a 360-degree range of detection.
													This is designed as an alternative of an expensive 360-degree LIDAR to lower the cost from $3000 to less than $300.
												</p>
											</div>
										</div>
										<div style="clear: both;"></div>
									</article>

									<article class="box excerpt">
										<a data-href="#" class="image left">
											<img class="embeded-youtube" src="./img/shakespeare.PNG">
										</a>
										<div>
											<header>
												<span class="date">A Geo-location Based Opinion Sharing Interface In Audio Format</span>
												<h3><a data-href="#">Post-it</a></h3>
											</header>
											<div>
												<p>
													The goal is to use real world information(personal voices and geo-locations) to augment the traditional text based post sharing system to encourage people to interact in a more personal way.
												</p>
												<p>
													An opinion sharing platform is designed for this purpose to help people to post their thoughts on Shakespeare's literature works.
												</p>
												<p>
													The platform takes audio recordings uploaded by people combined with the traditional text posts.
													It also helps people to show their geo-locations pinned on to a world map so it makes them know more about other peers and feel more comfortable sharing their own thoughts.
												</p>
											</div>
										</div>
										<div style="clear: both;"></div>
									</article>

									<article class="box excerpt">
										<a data-href="#" class="image left">
											<img class="embeded-youtube" src="./img/cementery.PNG">
										</a>
										<div>
											<header>
												<span class="date">Memorize the death in a digital way</span>
												<h3><a data-href="#">Digital Cemetery</a></h3>
											</header>
												<p>
													How can we memorize the dead in a generation that is use to digitalize almost anything into bits and bytes through cameras?
												</p>
												<p>
													This project is an experiment to provide a new way for people who wish to have their memories of their beloved families and friends in a digital manner.
													People are able to view the images of the tombs from maps of the cemeteries.
												</p>
												<p>
													170 tombs built 150 year ago located in Millbury Cemetery(Millbury MA, US) are uploaded in this way.
												</p>
											</div>
										<div style="clear: both;"></div>
									</article>


									<!--template-->
									<!--<article class="box excerpt">-->
									<!--<a data-href="#" class="image left">-->
									<!--<IFRAM></IFRAM>-->
									<!--</a>-->
									<!--<div>-->
									<!--<header>-->
									<!--<span class="date">short_des</span>-->
									<!--<h3><a data-href="#">xx</a></h3>-->
									<!--</header>-->
									<!--<div>-->

									<!--</div>-->
									<!--</div>-->
									<!--<div style="clear: both;"></div>-->
									<!--</article>-->
									<!--!template-->

								</section>

								<h2 class="bold">
									Motivation:
								</h2>
								<p>
									Interaction bridges from human to machine, human to information and human to human are my research interests.
								</p>
								<p>
									I believe interfaces should be powered by new 'sensing' technologies and they should be capable of learning through the interaction history data to encourage positive interactions and lower the cost and boundaries.
								</p>
								<p>
									I spent most of my college years on building interfaces between human and computers/robots.
									I have experimented on EEG and EMG technologies combined with voice control and built an interface that people who have no limb control can still have good control.
									This interface is adapted to control electrical wheelchairs, vehicle-robot-base and internet browsing experiences.

									I had the amazing opportunity to participate in projects like DARPA Humanoid Robot Project and NASA ROBO Ops.
								</p>

								<p>
									After graduation, I started to work in User Experience Development at Fidelity.
									Working there taught me a lot about how to build interfaces people are attracted to interact with through user research and in-person tests.
									I started to become interested in building new interfaces that encourage human interaction.
									<br>
									I experimented and developed a few web based applications.
									One of them is an opinion-sharing interface in audio format with the publishers’ geo-location linked on a world map.
									The goal was to create a different sharing experience which was geo-location based and audio formatted.
									With opinions posted on a geo-location based world map, it's amazing to see people start to post their different opinions based on their culture.
								</p>
								<p>
									Mixed reality control and interaction interfaces are my current focus. Based on audio interfaces in 3D spacial sounds I created a discovery-oriented language learning interface which simulated the natural learning experience.
									I built real-time in-person social interaction to help smooth the interaction between strangers by showing their profile information rendered next to their body following them in real time.
									This reduced the time cost of introductions and social ice-breaking so people can start a meaningful conversation without having to go through the somewhat awkward process of the initial greeting stage.
									By just seeing their interests directly, it effectively encouraged people to talk to others who share their common interests.
									I believe this type of interface will dramatically lower the cost between in-person communication in groups where people have much less knowledge of each other but wish to have enjoyable and meaningful conversations.
									I was fortunate enough to collaborate on a project, Mindful Wearable, born in the MIT Media Lab's Fluid Interface group, which creates mixed reality intervention on human behaviors.
								</p>
								<p>
									There's no deny that current AR hardwares including the physical size and computing powers are still a few years away from a complete adaptation in certain areas.
									But it's actually provides a perfect time and opportunity for expending and pushing the edge of understanding and applications in these areas.
								</p>
								<p>
									Interfaces should have memories of themselves in the future.
									The memory histories is the foundation of the AI for these interfaces.
									I experimented on integrating memories to control the interface on the wheelchair project.
									The control histories are parallel inputs just as the direct control inputs are.
									This could potentially remove the control noises for more accurate outcome for probability based interface controller.
								</p>

								<h2 class="bold">
									Bio
								</h2>
								<p>
									Worcester Polytechnic Institute (2010 – 2014), Worcester <br/>
									Bachelor's degree, Electrical and Computer Engineering && Minor Computer Science  <br/>
									GPA: 3.97/4.0
								</p>

								<p>
									Wayfair (2014-2015), Boston <br/>
									Software developer <br/>
								</p>

								<p>
									Fidelity User Experience Department, Boston (2015-2016) <br/>
									User Interface Developer <br/>
								</p>

								<h2 class="bold">
									Award:
								</h2>
								<p>
									Salisbury Prize Awards 2014:<br/>
									22 out of 1000 senior students of Worcester Polytechnic Institute<br/>
								</p>

								<h2 class="bold">
									Publication:
								</h2>
								<p>
									Augmenting a voice and facial expression control of a robotic wheelchair with assistive navigation<br/>
									2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)<br/>
									Authors: Dmitry A. Sinyukov, Ran Li, Runzi Gao, Nicholas W. Otero, Taşkın Padir
								</p>

								<h2 class="bold">
									Press
								</h2>
								<p class="italic"><a target="_blank" href="https://www.bostonglobe.com/business/2014/04/06/moving-wheelchair-raising-eyebrow/oSwdEWEvfs4XlKr4J9uAUM/story.html">
									At WPI, a push to make smart wheelchairs
								</a></p>

							</header>

							<hr>
						</div>
					</div>

					<footer id="footer">
						<div class="inner">
							<ul class="copyright">
								<li>&copy; Ran Li. All rights reserved</li>
							</ul>
						</div>
					</footer>

			</div>




		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="assets/js/main.js"></script>

	</body>
</html>
