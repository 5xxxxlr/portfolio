<!DOCTYPE HTML>
<html>
	<head>
		<title>Ran Li</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="assets/css/main.css" />
		<link rel="stylesheet" href="assets/css/customization.css" />
		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
	</head>
	<body>
		<!-- Wrapper -->
			<div id="wrapper">
					<div id="main">
						<div class="inner">
							<header>
								<h1>Ollo, </h1>
								<h1>My Name is Ran Li, always doing <a class="bold">idea engineering</a>.</h1>

								<h2 class="bold">
									PROJECTS:
								</h2>




								<section>
									<!-- Excerpt -->
									<article class="box excerpt">
										<a href="#" class="image left">
											<iframe class="embeded-youtube" src="https://www.youtube.com/embed/WzBv4E51Pgo" frameborder="0" allowfullscreen></iframe>
										</a>
										<div>
											<header>
												<span class="date">Mixed reality in-person social network assistance</span>
												<h3><a href="#">So-Show</a></h3>
											</header>
											<div>
												<p>
													It's a more intuitive way to interact with people in social events by mixing their digital social profile with real person figure in space.
												</p>
												<p>
													Editing digital profile will be reflected in mixed reality in real time through personal mobile devices.
												</p>
												<p>
													It creates a new social network experience in a world where real time digital information fulfills the unknown bits of the real world entities in mixed spaces.
												</p>

											</div>
										</div>
										<div style="clear: both;"></div>
									</article>

									<article class="box excerpt">
										<a href="#" class="image left">
											<iframe class="embeded-youtube" src="https://www.youtube.com/embed/Cr-W8ZpkiQs" frameborder="0" allowfullscreen></iframe>
										</a>
										<div>
											<header>
												<span class="date">Mixed reality language learning assistance</span>
												<h3><a href="#">Hololingo</a></h3>
											</header>
											<div>
												<p>An augmented reality application that helps students to learn language.</p>
												<p> The application is able to take voice input from teachers who setup the language learning goals by looking at the real world objects and binding pronunciations with them.

													After the setup stage, the students will try to look for these objects in real world by following the spatial sounds guidance.

													It is effective way of learning language vocabulary by creating the 'discovering' mindset in the real world with mixed reality.
												</p>
											</div>
										</div>
										<div style="clear: both;"></div>
									</article>

									<article class="box excerpt">
										<a href="#" class="image left">
											<iframe class="embeded-youtube" src="https://www.youtube.com/embed/q0ZK8wtPSw8" frameborder="0" allowfullscreen></iframe>
										</a>
										<div>
											<header>
												<span class="date">Behavior Interaction</span>
												<h3><a href="#">Mindful Wearable</a></h3>
											</header>
											<div>
												<p>
													A project that was born in MIT Media Lab, Fluid Interface group.
													This was the early prototype.
													It is aimed to analysis the visual input and providing certain intervention/guidance to the users.
												</p>
											</div>
										</div>
										<div style="clear: both;"></div>
									</article>

									<article class="box excerpt">
										<a href="#" class="image left">
											<iframe class="embeded-youtube" src="https://www.youtube.com/embed/XwkInPuPcG0" frameborder="0" allowfullscreen></iframe>
											<img class="embeded-youtube" src="./img/hospital_remote_control.png">
										</a>
										<div>
											<header>
												<span class="date">Hospital Remote Control Interface</span>
												<h3><a href="#">Home-care-robot</a></h3>
											</header>
											<div>
												<p>
													Robots that are capable of 'sensing' the patients' homes will be deployed to their houses after they are discharged from hospitals for follow-up medical care.
												</p>
												<p>
													The robots' 'sensing' system including a map-generation system that will generate maps of the patients' homes which enable a better observation for hostiple.
													The robots can also 'sense' the patients by a human-tracking system.
												</p>
												<p>
													Through the system, doctors are able to observe multiple patients who are at homes remotely without leaving hospitals and provide help in time.
												</p>
											</div>
										</div>
										<div style="clear: both;"></div>
									</article>


									<article class="box excerpt">
										<a href="#" class="image left">
											<iframe class="embeded-youtube" src="https://www.youtube.com/embed/Sb91fOYLG2g" frameborder="0" allowfullscreen></iframe>
											<iframe class="embeded-youtube" src="https://www.youtube.com/embed/Wqu-d__jWl4" frameborder="0" allowfullscreen></iframe>
											<iframe class="embeded-youtube" src="https://www.youtube.com/embed/syjQ7-KmUFg" frameborder="0" allowfullscreen></iframe>
										</a>
										<div>
											<header>
												<span class="date">EMG Human Computer Interfaces</span>
												<h3><a href="#">Adaptive EMG controller</a></h3>
											</header>
											<div>
												<p>
													Trying to replace traditional hand control interface such as joy stick,
													keyboard so people with no lime mobility can be better facilitated.
												</p>
												<p>
													Created multiple control signals through facial expressions via Emotive headset toolkit.
													Implemented modules to filter and process the signals from the devices output.
												</p>
												<p>
													Adapted interface modules to integrated signals to wheelchair control so people who can't use their limes(locked in syndrome) are still able to have certain mobility.

													The wheelchair users are able to navigate through the new interface in relatively complex environments.
												</p>
												<p>
													Adapted the interface modules to web browsing experience.
													It provides certain convenience to people who can't use mice and keyboards.
													Users are able to browser various predefined websites with certain navigation abilities.
												</p>

												<p>
													The same controller can also be easily adapted the interface modules to control robot base.
												</p>
											</div>
										</div>
										<div style="clear: both;"></div>
									</article>

									<article class="box excerpt">
										<a href="#" class="image left">
											<iframe class="embeded-youtube" src="https://www.youtube.com/embed/luStbnqcmEg" frameborder="0" allowfullscreen></iframe>
										</a>
										<div>
											<header>
												<span class="date">Voice Human Computer Interfaces</span>
												<h3><a href="#">Adaptive Voice controller</a></h3>
											</header>
											<div>
												<p>
													A different approach to replace the physical control interfaces(Joystick, keyboard, mouse)
												</p>
											</div>
										</div>
										<div style="clear: both;"></div>
									</article>

									<article class="box excerpt">
										<a href="#" class="image left">
											<iframe class="embeded-youtube" src="https://www.youtube.com/embed/ecN-d_ikPo8" frameborder="0" allowfullscreen></iframe>
											<img class="embeded-youtube" src="./img/darpa.png">
										</a>
										<div>
											<header>
												<span class="date">US Defense Advanced Research Projects Agency (DARPA) Robotics Challenge</span>
												<h3><a href="#">DARPA Atlas</a></h3>
											</header>
											<div>
												<p>
													Worked on the three-door task as part of the DARPA challenge 2013-2014.

													It requires the robot to locate three doors and open them then go through each one.
												</p>
												<p>
													Used point cloud data collected through a rotating 360 LIDAR sensor.
													The robot was able to identify the doors and their handles position.
													Then the human operators are able to control the robot to open and walk across the doors.
												</p>
											</div>
										</div>
										<div style="clear: both;"></div>
									</article>

									<!--template-->
									<!--<article class="box excerpt">-->
										<!--<a href="#" class="image left">-->
											<!--<IFRAM></IFRAM>-->
										<!--</a>-->
										<!--<div>-->
											<!--<header>-->
												<!--<span class="date">short_des</span>-->
												<!--<h3><a href="#">xx</a></h3>-->
											<!--</header>-->
											<!--<div>-->

											<!--</div>-->
										<!--</div>-->
										<!--<div style="clear: both;"></div>-->
									<!--</article>-->
									<!--!template-->


									<article class="box excerpt">
										<a href="#" class="image left">
											<img class="embeded-youtube" src="./img/low_cost_lidar.PNG">
										</a>
										<div>
											<header>
												<span class="date">A Low Cost Object Detection System</span>
												<h3><a href="#">LIDAR imitation</a></h3>
											</header>
											<div>
												<p>
													Build a system that can do a 360 degrees range of detection.
													This is designed as an alternative of an expensive 360 lidar to lower the cost for $3000 to less than $300.
												</p>
											</div>
										</div>
										<div style="clear: both;"></div>
									</article>

								</section>

								<h2 class="bold">
									Motivation:
								</h2>
								<p>
									Interaction bridges from human to machine, human to information and human to human are what I am building and let's call them digital interfaces!
								</p>
								<p>
									I believe interfaces should be powered by new technologies and they should be capable of learning through the interaction history data to encourage positive interactions and lower the cost and boundaries.
								</p>
								<p>
									I spend most of my college years on building interfaces between human and computer/robots.
									I have experimented on EEG, EMG technologies combined with voice control and build a interface that people who has no limb control can still have a good control.
									This interface are adapted to control eletrical-wheelchairs, vechecla-robot-base and internet browsing experience.

									I had the amazing opportunity to participate in project like DARPA Humanoid Robot Project and NASA ROBO Ops.
								</p>

								<p>
									After graduation, I started to work in a User Experience Development at Fidelity. , Working there taught me a lot on how to build interface people would love to interact with through user researches and testings pienrrred.
									and I start to become interested in building new interface can encourage human to interaction with information.
									<br>
									I developed an opinion sharing interface in audio format with the publishers' geo location linked on a world map. The goal was to create a different sharing experience which was geo-location based and audio formated.
									With opinions posted on geo-location based world map, it's amazingly to see people start to post their different opinion based on their culture.
								</p>

								<p>
									Mixed reality control and interaction interfaces are my current focus.
									Audio interfaces in 3D spacial sounds I created a discovery-oriented language learning interface which simulate the natural learning experience.
									I build real-time in-person social interaction to help smooth the interaction between strangers by showing their profile information rendered next to their body following them in real time.
									This reduced the time cost on introduction and social ice-break so people can start a meanful conversition without having to go through the somewhat awkward process of the inital greeting stage.
									By just seeing their interest directly, tt also effectively encouraged people to talk to the people who share these common interests.

									I believe this type of interfaces will dramaticly lower the cost between in-person communication in groups which people have very less knoweldge of each other but wish to have desired and meanful conversition.

									I was fortunate enough to collaborate on project, Mindful Wearable, born in MIT Media Lab in Fluid Interface group, which creates mixed reality intervention on human behaviors.
								</p>

								<p>
									Interface should have memories of themselves. The memory history should be an input factor feedback to the interfaces.
									I experimented on integrating memories to control the interface on the wheelchair project. The control histories are a parallel inputs just as the direct control input.
									This effectively removed a lot of the control noises for more accurate outcome.
								</p>
							</header>

							<hr>
						</div>
					</div>

					<footer id="footer">
						<div class="inner">
							<ul class="copyright">
								<li>&copy; Ran Li. All rights reserved</li>
							</ul>
						</div>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="assets/js/main.js"></script>

	</body>
</html>